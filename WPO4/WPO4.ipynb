{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f511eff26871492f3fa4ae077230fe51",
     "grade": false,
     "grade_id": "cell-f77284e87f75a760",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Health Information Systems and Decision Support Systems\n",
    "# WPO 4: - CAD Systems (20/03/2020)\n",
    "***\n",
    "*Panagiotis Gonidakis, Jakub Ceranka, Pieter Boonen, Jef Vandemeulebrouke*<br>\n",
    "*Department of Electronics and Informatics (ETRO)*<br>\n",
    "*Vrije Universiteit Brussel, Pleinlaan 2, B-1050 Brussels, Belgium*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Insert students name and IDs here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "35e56d1502506c7d95a2179be1975802",
     "grade": false,
     "grade_id": "cell-1180f9385da3b954",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Goal\n",
    "The goal of this practical session is to get an insight into artificial neural networks and convolutional neural networks. Your tasks will involve examining and preprocessing the data, training various neural networks and validating the performance of your system against the ground-truth predictions done manually by an experienced radiologist. Students must send their notebook, the image files representing the necessary graphs and the report in .ipynb and .html format. The grade from this practical session will contribute to your final grade.\n",
    "\n",
    "You are kindly requested to submit the <b> .ipynb </b>, an exported <b>.html</b> version  with all the cells properly executed and a <b>.zip</b> file containing all the <b>images</b> displaying the tensorboard training/validating curves </b>(in case are not displayed in the .ipynb).\n",
    "\n",
    "Please use the Canvas or Ufora assignment functionality to upload your reports.\n",
    "\n",
    "The deadline of the submission is in <b> 09/04/2020, 23:59 pm. </b>.\n",
    "\n",
    "If you have difficulties submitting your assignment, send it to [jceranka@etrovub.be](mailto:jceranka@etrovub.be) \n",
    "\n",
    "Questions: [jceranka@etrovub.be](mailto:jceranka@etrovub.be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5287e4bf438bc9b077ef9f2e4bcec518",
     "grade": false,
     "grade_id": "cell-65dd4b67d3695b43",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Libraries\n",
    "During this practical session, the following libraries will be used:\n",
    "\n",
    "* Numpy (np)\n",
    "* Glob\n",
    "* Simple ITK (sitk)\n",
    "* Matplotlib\n",
    "* Sklearn\n",
    "* Tensorflow (tf)\n",
    "* TFLearn / (you can also use keras)\n",
    "\n",
    "To import any external library, you need to import it using the **import** statement followed by the name of the library and the shortcut. You can additionally check for the module version using **version** command. \n",
    "\n",
    "* If you use your own laptop, you will need to install the two new modules. Otherwise, all the necessary software is already installed at the lab's PCs. \n",
    "\n",
    "* These expirements are simplified in order to be run without the need of a powerful GPU. However some training tasks may take 30-40 minutes using a CPU. You can accelerate your expirements if you work on [Google colab](https://colab.research.google.com/) framework where a GPU is offered. Then you need to create a GoogleDrive account and upload all the necessary data (scripts + data). \n",
    "For more information look [here](https://colab.research.google.com/) and [here](https://colab.research.google.com/notebooks/gpu.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "90dddf608397c1410372081b65393f39",
     "grade": false,
     "grade_id": "cell-cfeba9f5a5235a75",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### How to  install TFLearn\n",
    "\n",
    "TFLearn requires Tensorflow to be installed. For the purpose of this excercise you are promted to install Tensorflow with <b>CPU support only</b>.\n",
    "\n",
    "* https://www.tensorflow.org/install/ <p>\n",
    "* https://www.tensorflow.org/install/install_linux  <p>\n",
    "* https://www.tensorflow.org/install/install_mac  <p>\n",
    "* https://www.tensorflow.org/install/install_windows  <p>\n",
    "\n",
    "Then, install TFLearn: \n",
    "* http://tflearn.org/installation/\n",
    "\n",
    "Generally, open the anaconda prompt and then type:\n",
    " ```pip install tensorflow``` and after ```pip install tflearn```\n",
    " \n",
    "(*) Keep in mind that recently there is a new version of tensorflow. Depending on the python version you are working you might donwload tensorflow 1 or 2. For python 3.6, tensorflow 2.1.0 is by default is downloaded whereas for python 3.7, tensorflow 1.14.0. It is advised to verify which version it is being used before looking for any documentation as the APIs differ.\n",
    "\n",
    "(*) Alternally, if you have compatability issues or your are more familiar with keras, you can install [keras](https://keras.io/#installation) which is very similar to tflearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8f6b761e553767615fad950fc7d54345",
     "grade": false,
     "grade_id": "cell-e04efc9902fe123b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Lung Nodule Analysis \n",
    "Lung cancer is the leading cause of cancer-related deaths worldwide. Screening high risk individuals for lung cancer with low-dose CT scans is now being implemented in the United States and other countries are expected to follow soon. In CT lung cancer screening, millions of CT scans will have to be analyzed, which is an enormous burden for radiologists. Therefore, there is a lot of interest in development of computer algorithms to optimize cancer screening.\n",
    "\n",
    "<img src=\"images/luna16_image.png\">\n",
    "\n",
    "A vital first step in the analysis of lung cancer CT scans is the detection of pulmonary nodules, which may or may not represent early stage lung cancer. Many Computer-Aided Detection (CAD) systems have already been proposed for this task. \n",
    "\n",
    "The LIDC/IDRI data set is publicly available, including the ground-truth annotations of nodules performed by four radiologists.\n",
    "\n",
    "This practical session is inspired from the challenge [LUNA16](https://luna16.grand-challenge.org/home/), which focused on a large-scale evaluation of automatic nodule detection algorithms on the LIDC/IDRI data set.\n",
    "\n",
    "<img src=\"images/lung_cancer1.png\">\n",
    "\n",
    "\n",
    "### Using LIDC/IDRI data set in this practical session\n",
    "\n",
    "For the needs of this practical session, we will look for an algorithm that only determines the likelihood for a given location in a CT scan to contain a pulmonary nodule. Furthermore, we have included a very small part of the LIDC/IDRI data set and we will use only a slice of suspicious regions of a CT scan.\n",
    "\n",
    "### Data augmentation\n",
    "\n",
    "Originally our dataset was very unbalanced. There were a lot samples of non-nodules (negatives) but very few samples of nodules (positives). Machine learning algorithms and specifically neural networks and convolutional neural networks require to be trained on balanced dataset, meaning all the classes should be equally represented in the training set. \n",
    "\n",
    "Using data augmentation methods (**rotation and translation**), positive samples were massively augmented in order to balance the two classes (nodules and non-nodules)\n",
    "\n",
    "### Ground Truth data\n",
    "\n",
    "Categorical data are variables that contain label values rather than numeric values. In our dataset, a sample can represent a nodule or a non-nodule area, so initially we have our ground truth data in a categorical form. Many machine learning algorithms cannot operate on label data directly. They require all input variables to be numeric. This means that categorical data must be converted to a numerical form. This involves two steps:\n",
    "1. Integer Encoding\n",
    "2. One-Hot Encoding.\n",
    "\n",
    "As a first step, each unique category value is assigned an integer value. That's why in our dataset, a sample which represents a nodule will have as a label **1** and a sample which represents a non-nodule area will have as a label **0**.\n",
    "\n",
    "For our case, this enconding step would be enough since we have only two categories. However, in a more general problem with more than two classes, using this encoding allows the model to assume a natural ordering between categories which may result in poor performance or unexpected results. That's why, one hot encoding can be appled to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value. \n",
    "\n",
    "Therefore, a  nodule will be represented by the binary variable [1,0] and a non-nodule area by the binary variable [0,1].\n",
    "\n",
    "<img src=\"images/lung_cancer2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "77b5ce922c11056fda2cc0a605361876",
     "grade": false,
     "grade_id": "cell-88edf7d56c17540b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 1: Visualize, load and prepare data for Machine Learning Algorithms\n",
    "\n",
    "CT images are stored in MetaImage (mhd/raw) format. Each .mhd file is stored with as a header file (.mhd) and a corresponding pixeldata file (.raw). To load a CT image, just **load the .mhd header file** and data from the binary .raw file will be automatically loaded.\n",
    "\n",
    "If you look carefully at the name of each .mhd file, you can extract useful information for a specific sample. You can identify its **number id**, its **size**, if the image was produced by a **data augmentation** method and if it contains a **nodule or not**.\n",
    "\n",
    "For example: *20046_x0y0z0_20x20x6_r0_1.mhd*\n",
    "* **20046**:   number of candidate patch\n",
    "* **x0y0z0**: no translation in any axis (if augmentation is used it is mentioned by the angle in the corresponding axis)\n",
    "* **20x20x6**: size of the image in voxels\n",
    "* **r0**:     no rotation\n",
    "* **1**:      it is a positive sample - represents a nodule\n",
    "* **.mhd**:   it is a mhd file (this is the file which can be loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "eaa451a946fc33d891799f2910eb06af",
     "grade": false,
     "grade_id": "cell-6e65384d14dac861",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 1: Import necessary libraries\n",
    "\n",
    "Load all necessary libraries using the **import** statement and check for errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 1: Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "945c2ee4613c085addad9e466a2e2d03",
     "grade": false,
     "grade_id": "cell-1f6fe6d45fa0e86d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 2: Inspect the dataset\n",
    "\n",
    "1. Using SimpleITK, create a function to load .mhd files (__Tip:__ The function should return a numpy array - sitk.GetArrayFromImage( ) )\n",
    "2. Visualize the 6 slices of a chosen patch using matplotlib subplot figure and mention if it is a positive or a negative patch.\n",
    "3. Visualize some augmented samples from the same candidate region using matplotlib subplot of the same patch and mention the augmentation method. Check visually if the observed patch was modified using the same augmentation method that is mentioned in the samples' filename.\n",
    "4. Count your files. How many positives and negatives there are in this dataset? (__Tip:__ Use glob library to get the number of specific files in your dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 1: Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dcf46ab947dd55a873e1469c775a2f59",
     "grade": false,
     "grade_id": "cell-d9e7fbe181dfe944",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 3: Data pre-processing\n",
    "In order to use neural-networks, we need to pre-process the data and store it in a way which can be easily interpreted by tensorflow.\n",
    "\n",
    "First of all, separate the test set from the train set. Keep 10% of your samples as a test-set. (**Tip:** Use sklearn **train_test_split** function.)\n",
    "\n",
    "Since 3D CNNs need a lot of computation power, we will convert our problem to 2D. Instead of using all the 6 CT slices, we will use only the 3rd CT slice. Therefore, from each sample, the 3rd slice wiil be extracted and further preprocessed. In addition, our values currently range from -1024 HU to around 2000 HU. Anything above 400 is not interesting to us, as these are simply bones with different radiodensity. A commonly used set of thresholds to normalize between is -1000 and 400. Finally the labels will be properly encoded to be used for training and testing.\n",
    "\n",
    "Write your pre-processing tasks as python functions (listed below) and in the end create a pipeline for each sample which will be also implemented as a function.\n",
    "1. Function loading the .mhd image as a numpy array.\n",
    "2. Function extracting the 3rd slice of a patch. The final form of patches should be a numpy array of size **20x20** pixels.\n",
    "3. Function normalizing the dataset.\n",
    "The unit of measurement in CT scans is the **Hounsfield Unit (HU)**, which is a measure of radiodensity. CT scanners are carefully calibrated to accurately measure this. From Wikipedia:\n",
    "<img src=\"images/HU_CTscannersCalibration.png\">\n",
    "\n",
    "    Create a function which is going to normalize the samples according to this table \n",
    "        * Create numpy arrays \n",
    "        * Normalize between [-1000, 400] using this normalization method \n",
    "    $npzarray = (npzarray - minHU) / (maxHU - minHU)$ <br>\n",
    "         where minHU = -1000 and maxHU = 400\n",
    "        * After this normalization set any values bigger than 1 to 1 and any values smaller than 0 to 0. \n",
    "\n",
    "4. If statement creating **1-hot** labels as a ground truth data in order to train the neural networks. (One-hot labels: [0,1] --> negative, [1,0]--> positive)\n",
    " \n",
    "5. Store into .npy data binary files. (__Tip:__ Use np.save to store data.)\n",
    "\n",
    "\n",
    "The pipeline should execute the functions as follows: <br>\n",
    "Load mhd using sitk -> normalize -> Extract 3rd slice -> Label \n",
    "\n",
    "and return:\n",
    "* data:  [sample_idx,20,20]\n",
    "* labels: [sample_idx,2]\n",
    "\n",
    "Data pre-processing should be done for both: train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 1: Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f0f3e11c93d2004ee46bad5992f6c714",
     "grade": false,
     "grade_id": "cell-f4edb7482a0753c4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In case you are not able to complete part 1, you can move to part 2 of the session by loading the provided .npy files. In that case you will **not** get any credits for Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "687da3742ff425ca860c8478ed7d081a",
     "grade": false,
     "grade_id": "cell-d1fb33251b563bb8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 2: Model training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3d1de1e961e4781cd28635929dc63f5d",
     "grade": false,
     "grade_id": "cell-a11370ef4ea445ca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Using TFLearn - A High Level API for Tensorflow\n",
    "\n",
    "TFLearn introduces a High-Level API that makes neural network building and training fast and easy. This API is intuitive and fully compatible with Tensorflow.\n",
    "\n",
    "Have a look at the following links for more information about TFLearn: <br>\n",
    "http://tflearn.org/getting_started/ <p>\n",
    "http://tflearn.org/tutorials/quickstart.html <p>\n",
    "http://tflearn.org/models/dnn/ <p>\n",
    "\n",
    "When you install TensorFlow, you automatically also install <b>Tensorboard</b>. During or after training you can use  it  to visualize the network and its performance. For the next tasks, <b>include both the loss and the accuracy graphs for training and also for evaluating the test-set</b>. A simple way to attach these graphs to jupyter notebook is to use Window's Snipping Tool or a similar application. Then display it either using a plain HTML ```<img src>``` or by using ```IPython.display.Image``` and ```IPython.core.display.HTML```. Don't forget to submit the actual image file with your .ipynb as the image is not saved in the notebook, it is just linked.\n",
    "\n",
    "### Some Tips:\n",
    "\n",
    "* Use ```np.load``` to load your data from the previously created .npy binary files.\n",
    "* Use ```reshape``` method when it is needed to import data to the network.\n",
    "* Make your own function to create the model for each network architecture to avoid any model conflicts.\n",
    "* To define a model:\n",
    "```python\n",
    " network = ...(some layers) ...\n",
    " \n",
    " network = regression(network, optimizer='...', loss='...')\n",
    " modelname = DNN(network) \n",
    "```\n",
    "* Use the model's method <b>fit</b> to train your network:\n",
    "```python\n",
    " modelname.fit(X,Y)\n",
    "```\n",
    "\n",
    "* Use the model's method <b>save</b> to save your trained model:\n",
    "```python\n",
    " modelname.save('modelname.tflearn')\n",
    "```\n",
    "\n",
    "\n",
    "* Use the model's methods <b>load</b> to load a model from a previous python session.\n",
    "```python\n",
    " modelname.load('modelname.tflearn') \n",
    "```\n",
    "\n",
    "* If you trained a model in a past python session, you can load the trained model but first you have to <b>define and initialize it again</b>. \n",
    "```python\n",
    " network = ...\n",
    " \n",
    " modelname.DNN(network)    \n",
    " modelname.load('modelname.tflearn')\n",
    " modelname.predict(X)\n",
    "```\n",
    "\n",
    "\n",
    "* Define the directory to store the tensorboard log files by adding it as a parameter when initializing the model.\n",
    "```python\n",
    " tflearn.models.dnn.DNN (network, clip_gradients=5.0, tensorboard_verbose=0, \n",
    " tensorboard_dir='/tmp/tflearn_logs/', checkpoint_path=None, best_checkpoint_path=None, max_checkpoints=None, session=None, best_val_accuracy=0.0)\n",
    "```\n",
    "* Use <b>Tensorbaord</b> to visualize network and performance:\n",
    "Open the command prompt or the anaconda prompt (or a terminal for linux/iOS) and in the working directory type:\n",
    "``` $ tensorboard --logdir='...' ```\n",
    "* You may need to add tensorboard's script to your SYSTEM'S PATH to be able to launch it. \n",
    "* Use model's method <b>predict</b> to evaluate a test sample. \n",
    "```python \n",
    "modelname.predict(X) ```\n",
    "\n",
    "* <b>If you are using the tensorflow 2 it is possible to run tensorboad using the jupyter notebook. </b>\n",
    "In a jupyter cell type: ``` %tensorboard --logdir logs ```.\n",
    "For more information look [here](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks)\n",
    "\n",
    "* If you have strange tflearn errors, restart the python kernel and <b> re-run only the cells concerning your task you currently work (i.e. run the cells concerning only the current tfmodel). Sometimes you cannot load two different tfmodels at the same python session.</b>.\n",
    "\n",
    "(*) Depending the version of tflearn/tensorflow you are working some commands might differ. Please look online for recent documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8d6d8b691a25bac726e81fb9f5593de9",
     "grade": false,
     "grade_id": "cell-e3701a8ed58211ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Some Tensorboard examples.\n",
    "Here are some examples of a training session. After each task, expand accuracy and loss graphs and attach them in the next cell of the notebook like we did in this exmple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6d3fdb9f0dbaf340f3a3cb972204745a",
     "grade": false,
     "grade_id": "cell-3399b1ef562b5cc9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<img src=\"screenshots_tensorboard/all_graphs.png\">\n",
    "<img src=\"screenshots_tensorboard/accuracy_graph.png\">\n",
    "<img src=\"screenshots_tensorboard/loss_graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "839c62166cf59b1aba0ebb12f7e38d6d",
     "grade": false,
     "grade_id": "cell-d5025d3539123bdb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 1: Load Data from the numpy binary files (from Part 1)\n",
    "Use the numpy load function to load the data created in the previous part of the exercise. The data has to be reshaped in a specific way (see code below) in order to be compatible with the tflearn neural network input standard. After the data is successfully loaded, plot any patch and its ground truth as image title to verify that data is correctly represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 1\n",
    "\n",
    "X = train_data.reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "X_val = test_data.reshape(-1,IMG_SIZE,IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d1106dc6ff56598fb4c8afd41d9061f",
     "grade": false,
     "grade_id": "cell-0afd5980978d0695",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 2: Artificial Neural Networks (1 layer)\n",
    "First, lets train our classifier using an artificial neural network with just one layer, a fully connected one. This layer will have *softmax* as activation function. \n",
    "\n",
    "The main advantage of using this activation function is the output range. The range of the outputs will be [0,1] and the sum of all the probabilities will be equal to one. Generally, if the softmax function is used for a  multi-classiication model it returns the probabilities of each class and the target class will have the high probability.\n",
    "\n",
    "Using TFLearn, define your model. Use these hyperparameters:\n",
    "* Learning Rate = 1e-4\n",
    "* Batch Size = 8\n",
    "\n",
    "Since the network is small, it is not recommended to use Dropout. Train first for 1-2 epochs to verify that everything works and then train for ~10 epochs. Observe the training and evaluate graphs from Tensorboard to decide when to stop training. Copy these graphs to your notebook and explain what happens.\n",
    "\n",
    "To evaluate the trained model, test it with the test samples and use various performance measures, like <b>confusion matrix</b> for different decision thresholds, <b>precision/recall</b> versus the decision threshold graph and finally plot a **ROC** curve. \n",
    "\n",
    "Explain these graphs and the ROC curve. What role does the decision threshold play and why it is so important? Can we have a single value as the decision threshold? Why?\n",
    "\n",
    "### Tip:\n",
    "* It is highly recommended to write all the tasks in functions and have as an input argument your tfmodel. Then you will be able to re-use them in the next tasks for different tfmodels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "729112bb739cb27832f2c27f1d3d3c5a",
     "grade": false,
     "grade_id": "cell-ec3cbbcde6634b2d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 3: Test the model\n",
    "Use **predict** method to test a single test sample. Plot the patch and include in the title its ground truth and the prediction. Now, test all the test samples using a for loop and print the accuracy for a decision threshold of 0.5. Additionally, print the **confusion matrix** for the same decision threshold and comment on your findings. Do you think it's necessary to investigate our system's performance by setting different decision thresholds? What performance measurement should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c2d112b399d5f76d26eb16384424a219",
     "grade": false,
     "grade_id": "cell-bfdd83f2ba53424c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Print the ROC curve and comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 3 - ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d82ad9587727760dd790c5846e9ba05f",
     "grade": false,
     "grade_id": "cell-ec462a6ad2168c48",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 4.1: Artificial Neural Networks (3 layers) - Let's go deep!\n",
    "\n",
    "Let's try 3 fully-connected layers using **sigmoid** activation function. Choose different values for the learning rate, number of epochs and batch size (e.g. 10, 20, 30...). \n",
    "\n",
    "Here is an example network but you are free to choose your own network.\n",
    "* 1st FC with 80 neurons, sigmoid/relu\n",
    "* 2nd FC with 40 neurons, sigmoid/relu\n",
    "* 3rd FC with 2 neurons and softmax\n",
    "* optimizer: sgd or adam or ...\n",
    "\n",
    "Explain your network's architecture and **test** your system as you did for the previous simple neural network. What do you observation? Print some graphs which indicate **underfitting** and **overfitting**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "822f2a649d5f2f9cebf5cfb29e5681f1",
     "grade": false,
     "grade_id": "cell-2f4b91a56afdcffa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 4.2: Relu!\n",
    "RELU = Rectified Linear Unit\n",
    "\n",
    "Let's try the same network but using RELU instead of sigmoid now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9daeec5dcb3d381740aeac3613a7b8ba",
     "grade": false,
     "grade_id": "cell-a6394cf303cb2289",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 4.3: Dropout!\n",
    "Use dropout. Some recommended values to investigate are 0.9, 0.5, 0.1. Explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c7c0f2e845aafc10ec8ad052d48e9119",
     "grade": false,
     "grade_id": "cell-2df072bbe1906a59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 5. A Convolutional Neural Network\n",
    "\n",
    "#### Model definition\n",
    "\n",
    "input -> conv1 -> pool1 -> conv2 -> fc1 -> fc2 -> softmax\n",
    "* convolution layers: kernel size = 3x3, number of channels = 64, activation function = sigmoid\n",
    "* max-pooling layers: kernel size = 2x2, strides = 2\n",
    "* 1st fully connected: 50 neurons, activation function = sigmoid\n",
    "* 2nd fully connected: 2 neurons, activation function = softmax\n",
    "* optimizer = sgd / adam\n",
    "* use dropout (e.g. 0.5)\n",
    "\n",
    "Like before, measure system's prerfomance and explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c7c0f2e845aafc10ec8ad052d48e9119",
     "grade": false,
     "grade_id": "cell-2df072bbe1906a59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 6. A Deeper Convolutional Neural Network\n",
    "\n",
    "#### Model definition\n",
    "\n",
    "input -> conv1 -> pool1 -> conv2 -> conv3 -> fc1 -> fc2 -> softmax\n",
    "* convolution layers: kernel size = 5x5, number of channels = 64, activation function = relu\n",
    "* max-pooling layers: kernel size = 2x2, strides = 2\n",
    "* 1st fully connected: 100 neurons, activation function = relu\n",
    "* 2nd fully connected: 2 neurons, activation function = softmax\n",
    "* optimizer = sgd / adam\n",
    "* use dropout (e.g. 0.5)\n",
    "\n",
    "Like before, measure system's prerfomance and explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c7c0f2e845aafc10ec8ad052d48e9119",
     "grade": false,
     "grade_id": "cell-2df072bbe1906a59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 7\n",
    "\n",
    "\n",
    "Comment the network differences between the two convolutional neural networks. Which ones do you think have the strongest impact in the network's perfomance? Justify your answer. \n",
    "\n",
    "An implementation of one or more networks might be necessary to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Code for Part 2: Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d0ec0155912f600620327325c499bac",
     "grade": false,
     "grade_id": "cell-72468f1cc2255060",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### (BONUS) Create your own convolution neural network\n",
    "\n",
    "Create your own network and try to increase model's accuracy. You can use different:\n",
    "* size of convolutional kernels\n",
    "* activation functions\n",
    "* optimizers ...\n",
    "***\n",
    "\n",
    "* **Extra task**: Select a different part of the dataset as test-set and repeat the training tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for BONUS part"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
